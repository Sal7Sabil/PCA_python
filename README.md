# PCA_python
PCA is transformation of a high dimensional vector space into low dimensional space. In real-world, we usually get data sets which are beyond 2 or 3 dimensions. It is difficult for us to visualise and comprehend them as it has more than 100 features. Training machine learning models become difficult, time-consuming and very expensive to run. 

In this algorithm, we filter out mathematically which features are important and which are not (feature extraction). PCA orthogonally transforms the n coordinates called “principal components”. The result is the first principal component, the component with the largest variance and is orthogonal (i.e. uncorrelated) with the previous components. 

# Screenshots

<p float="left">
<img src="https://github.com/Sal7Sabil/PCA_python/blob/main/PCA_manual.png" width="350" height="350" />
<img src="https://github.com/Sal7Sabil/PCA_python/blob/main/PCA_sklearn.png" width="350" height="350" />
</p>

# Libraries used

Matplotlib <br>
Sklearn
